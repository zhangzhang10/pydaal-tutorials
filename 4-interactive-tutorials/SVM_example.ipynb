{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Boilerplate\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "# Import customUtils\n",
    "sys.path.append(os.path.realpath('../3-custom-modules'))\n",
    "from customUtils import getArrayFromNT\n",
    "\n",
    "# Intel DAAL related imports\n",
    "from daal.data_management import HomogenNumericTable\n",
    "\n",
    "# Import numpy, matplotlib, seaborn\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting configurations\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwritten Digits Recognition with SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab is partially based on a [scikit-learn tutorial](http://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html) on the same subject."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial brief\n",
    "This lab studies a popular and powerful classification algorithm, _Support Vector Machine_ (SVM). We build a multi-class classifier on top of SVM using PyDAAL, and use it to tackle a famous classification problem in machine learning: handwritten digits recognition. We go on to solve the same problem using scikit-learn's Support Vector Classifier, and compare performance and classification accuracy between the PyDAAL solution and the scikit-learn solution.\n",
    "\n",
    "The lab code contains definition of a `MultiClassSVM` class, which consists of a `train` method and a `predict` method, among other helper functions. Except for the `predit` method, all parts of the class is implemented. In the exercise, you are asked to implement the `predict` method.\n",
    "\n",
    "### Learning objectives\n",
    "* To understand and practice the typical code sequence of using PyDAAL for classification.\n",
    "* To see PyDAAL's performance advantage over scikit-learn.\n",
    "\n",
    "### SVM introduction\n",
    "SVM belongs to a family of generalized linear classification problems. It is a binary (two-class) classifier. It is typically used a key ingredient of a multi-class classifier to address multi-class classification problems.\n",
    "\n",
    "According to Wikipedia [link](https://en.wikipedia.org/wiki/Support_vector_machine):\n",
    "\n",
    "> An SVM model is a representation of the examples as points in space, mapped so that the examples of the separate categories are divided by a clear gap that is as wide as possible. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall on.\n",
    "\n",
    "Despite its origin as a linear classifier, SVM is often used for non-linear classification problems. The key is the _kernel functions_. A kernel function maps features to higher dimensional feature spaces, making SVM to be able to capture non-linear relations. The SVM in DAAL supports two kernel functions: linear and KBF (Gausian kernel).\n",
    "\n",
    "### Multi-class classifier introduction\n",
    "SVM by itself is a binary (two-class) classifier. To use it on multi-class problems, DAAL employs a technique called One-Against-One. In plain language, let $K$ be the number of classes, the One-Against-One approach solves a two-class problem for each possible pair of labels of the $K$ labels. Then, the predicted label is the one that was predicted by the majority of the two-class classifiers.\n",
    "\n",
    "### The handwritten digits dataset\n",
    "\n",
    "Scikit-learn has some functions to load popular datasets for eager learners. These datasets are available through [sklearn.datasets](http://scikit-learn.org/stable/datasets). The [load_digits](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits) method loads and returns the digits dataset. Because the dataset internally uses NumPy ndarray to store information, we can convert it to DAAL `NumericTables`, and pass them to DAAL algorithms. \n",
    "\n",
    "After loading the data, we take a quick look at the sizes and dimensions. There are 1797 samples (i.e. images of handwritten digits) in the dataset, and each sample has 64 features. Note that the images are stored in `digits.data` and the corresponding labels are stored in `digits.target`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "print(digits.data.shape)\n",
    "print(digits.target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the first 10 images and their correct labels (ground truth). The code below is directly copied from the scikit-learn tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images_and_labels = list(zip(digits.images[:10], digits.target[:10]))\n",
    "for index, (image, label) in enumerate(images_and_labels):\n",
    "    plt.subplot(2, 5, index + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title('Training: %i' % label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "We save the last 100 samples from the dataset for testing, and use the rest to train a classifier.\n",
    "\n",
    "As described above, images and the corresponding labels are accessible in `digits.data` and `digits.target`, respectively. The code below creates four Intel DAAL numeric tables, for `training_data`, `training_labels`, `test_data`, and `test_labels`.\n",
    "\n",
    "It is important to keep in mind that DAAL NumericTables can only be created from ndarrays with C-contiguous memory layout. `digits.data` and `digits.target` are not C-contiguous. You can check this with:\n",
    "\n",
    "```python\n",
    "digits.data.flags['C']\n",
    "```\n",
    "To put them into correct memory layout, we use NumPy function [`np.ascontiguousarray`](http://docs.scipy.org/doc/numpy/reference/generated/numpy.ascontiguousarray.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the data into training data and labels, and create numeric tables\n",
    "nsamples = len(digits.images)\n",
    "data = np.ascontiguousarray(digits.data, dtype = np.double)\n",
    "labels = np.ascontiguousarray(digits.target.reshape(nsamples,1), dtype = np.double)\n",
    "\n",
    "training_data = HomogenNumericTable(data[:-100])\n",
    "training_labels = HomogenNumericTable(labels[:-100])\n",
    "\n",
    "test_data = HomogenNumericTable(data[-100:])\n",
    "test_labels = HomogenNumericTable(labels[-100:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a multi-class classifier based on SVM\n",
    "\n",
    "The definition of the `MulticlassSVM` class using PyDAAL is below. A few things to help understanding the code:\n",
    "\n",
    "1. The implementation uses `multi_class_classifier` from PyDAAL, which can be parameterized by an underlying two-class classifier, and the number of classes. SVM is hard-coded as the two-class classifier, while the number of classes is a user input.\n",
    "2. The SVM classifier itself takes several parameters, in particular,\n",
    "    * Kernel function: Either a linear kernel or an RBF kernel. A kernel function also has its own parameters. For example, the RBF kernel is parameterized by $\\sigma$.\n",
    "    * Cache size (in bytes): A cache is used to store the kernel matrix. For best performance, the cache size should be about `number_of_samples x number_of_samples x sizeof(feature_data_type)`.\n",
    "    * C: Upper bound in conditions of the quadratic optimization problem. It is used to control the trade-off between variance and bias of the model. It is typically set to 1.0.\n",
    "    * shrinking: A bool value that enables or disables kernel shrinking. Kernel shrinking is an optimization technique to reduce the amount of kernel computation.\n",
    "    * There are other parameters, such as accuracy threshold and $\\tau$ the parameter for the WSS scheme, that are not explicitly set in the code below. We just use their default values. For details about what these parameters are about, refer to the [Intel DAAL Developer Guide](https://software.intel.com/sites/products/documentation/doclib/daal/daal-user-and-reference-guides/index.htm).\n",
    "3. The `predict` method of the class is left as an exercise. You should follow the `train` method as an example to flesh out the implementation for `predict`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from daal.algorithms.svm import training as svm_training\n",
    "from daal.algorithms.svm import prediction as svm_prediction\n",
    "from daal.algorithms.kernel_function import linear, rbf\n",
    "from daal.algorithms.multi_class_classifier import training as multiclass_training\n",
    "from daal.algorithms.multi_class_classifier import prediction as multiclass_prediction\n",
    "from daal.algorithms.classifier import training as training_params\n",
    "from daal.algorithms.classifier import prediction as prediction_params\n",
    "\n",
    "class MulticlassSVM:\n",
    "\n",
    "\n",
    "    def __init__(self, nclasses):\n",
    "        \"\"\"Initialize class parameters\n",
    "\n",
    "        Args:\n",
    "           nclasses: number of classes\n",
    "        \"\"\"\n",
    "\n",
    "        self._nclasses = nclasses\n",
    "        # Create an SVM two-class classifier object for training\n",
    "        #---self._svm_training_alg = svm_training.Batch_Float64DefaultDense()\n",
    "        self._svm_training_alg = svm_training.Batch()\n",
    "        # Create an SVM two-class classifier object for prediction\n",
    "        self._svm_prediction_alg = svm_prediction.Batch_Float64DefaultDense()\n",
    "\n",
    "    \n",
    "    def setSVMParams(self, \n",
    "            cachesize = 1000000000, \n",
    "            C = 1.0,\n",
    "            sigma = 1.0,\n",
    "            kernel = linear.Batch_Float64DefaultDense(),\n",
    "            shrinking = False):\n",
    "        \"\"\"Tweak SVM training and prediction algorithm parameters\n",
    "\n",
    "        Args:\n",
    "            cachesize: size of chache in bytes for storing kernel matrix\n",
    "            kernel: SVM kernel, can be either linear or rbf\n",
    "            sigma: Coefficient of the rbf kernel\n",
    "            shrinking: whether do shrinking optimization or not\n",
    "        \"\"\"\n",
    "\n",
    "        self._svm_training_alg.parameter.cacheSize = cachesize \n",
    "        self._svm_training_alg.parameter.C = C \n",
    "        if getattr(kernel.parameter, 'sigma', None):\n",
    "            kernel.parameter.sigma = sigma\n",
    "        self._svm_training_alg.parameter.kernel = kernel\n",
    "        self._svm_prediction_alg.parameter.kernel = kernel\n",
    "        self._svm_training_alg.parameter.doShrinking = shrinking\n",
    "\n",
    "\n",
    "\n",
    "    def train(self, data, labels):\n",
    "        \"\"\"Train an SVM model.\n",
    "\n",
    "        Args:\n",
    "            data: training data\n",
    "            labels: ground truth known for training data \n",
    "\n",
    "        Returns:\n",
    "            An SVM model object\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create a multiclass classifier object based on the\n",
    "        # SVM two-class classifier\n",
    "        multiclass_training_alg = multiclass_training.Batch_Float64OneAgainstOne()\n",
    "        multiclass_training_alg.parameter.nClasses = self._nclasses\n",
    "        multiclass_training_alg.parameter.training = self._svm_training_alg\n",
    "        multiclass_training_alg.parameter.prediction = self._svm_prediction_alg\n",
    "\n",
    "        # Pass training data and labels\n",
    "        multiclass_training_alg.input.set(training_params.data, data)\n",
    "        multiclass_training_alg.input.set(training_params.labels, labels)\n",
    "\n",
    "        # Build the model and return it\n",
    "        return multiclass_training_alg.compute().get(training_params.model)\n",
    "\n",
    "    \n",
    "\n",
    "    def predict(self, model, testdata):\n",
    "        \"\"\"Make predictions for unseen data using a learned model.\n",
    "\n",
    "        Args:\n",
    "            model: a learned SVM model\n",
    "            testdata: new data\n",
    "\n",
    "        Returns:\n",
    "            A NumericTable containing predicted labels\n",
    "        \"\"\"\n",
    "\n",
    "        # Create a multiclass classifier object based on the\n",
    "        # SVM two-class classifier\n",
    "        #\n",
    "        # YOUR CODE HERE\n",
    "        #\n",
    "        # The multi-class prediction algorithm you need is Batch_Float64MultiClassClassifierWuOneAgainstOne\n",
    "        # Follow the example in the `train` method to set parameters, including nClasses, and training \n",
    "        # and prediction algorithms for the underlying two-class classifier. \n",
    "        multiclass_prediction_alg = multiclass_prediction.Batch_Float64MultiClassClassifierWuOneAgainstOne(self._nclasses)\n",
    "        \n",
    "        multiclass_prediction_alg.parameter.training = self._svm_training_alg\n",
    "        multiclass_prediction_alg.parameter.prediction = self._svm_prediction_alg\n",
    "        # Pass a model and input data\n",
    "        #\n",
    "        # YOUR CODE HERE\n",
    "        #\n",
    "        # Use the input.setModel method to specify a pre-trained model. The input ID to use is\n",
    "        # prediction_params.model.\n",
    "        # Use the input.setTable method to specify test data. The input ID to use is prediction_params.data\n",
    "\n",
    "        multiclass_prediction_alg.input.setTable(prediction_params.data, testdata) \n",
    "        multiclass_prediction_alg.input.setModel(prediction_params.model, model) \n",
    "\n",
    "        # Compute and return prediction results\n",
    "        #\n",
    "        # YOUR CODE HERE\n",
    "        results = multiclass_prediction_alg.compute()\n",
    "    \n",
    "        #\n",
    "        # Call the `compute` method of the multi-class prediction algorithm. Store the return value into \n",
    "        # variable `results`.\n",
    "     \n",
    "        return results.get(prediction_params.prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the `MulticlassSVM` is fully implemented, we can apply it to the handwritten digits recognition problem.\n",
    "\n",
    "The code below creates a `MulticlassSVM` object, sets some parameters, and then continues to train a model using the training data and training labels we defined above. Next, the model is used to make predictions on the test data. We can time the training and prediction stages, respectively. We will compare the timings with that of the scikit-learn SVC solution later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from svm_multi_class import *\n",
    "\n",
    "nclasses = 10\n",
    "classifier = MulticlassSVM(nclasses)\n",
    "\n",
    "classifier.setSVMParams(\n",
    "        cachesize = 32000000,\n",
    "        kernel = linear.Batch_Float64DefaultDense(),\n",
    "        shrinking = True)\n",
    "\n",
    "%time svm_model = classifier.train(training_data, training_labels)\n",
    "\n",
    "%time predictions = classifier.predict(svm_model, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality metrics\n",
    "We can check the performance of the model by computing quality metrics. DAAL's algorithms offer quality metrics libraries to evalute both binary and multi class classfier. Below is `ClassifierQualityMetrics` class constructed using DAAL's low level API that has methods put together to compute quality metics for both binary class(`computeTwoclassQualityMetrics`) and multi-class(`computeMulticlassQualityMetrics`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from daal.algorithms.multi_class_classifier import quality_metric_set as multiclass_quality\n",
    "from daal.algorithms.classifier.quality_metric import multiclass_confusion_matrix \n",
    "from daal.algorithms.svm import quality_metric_set as twoclass_quality\n",
    "from daal.algorithms.classifier.quality_metric import binary_confusion_matrix\n",
    "from daal.data_management import BlockDescriptor_Float64, readOnly\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "# Two-class quality metrics type\n",
    "TwoClassMetrics = namedtuple('TwoClassMetrics',\n",
    "        ['accuracy', 'precision', 'recall', 'fscore', 'specificity', 'auc'])\n",
    "\n",
    "# Multi-class quality metrics type\n",
    "MultiClassMetrics = namedtuple('MultiClassMetrics',\n",
    "        ['accuracy', 'error_rate', 'micro_precision', 'micro_recall',\n",
    "         'micro_fscore', 'macro_precision', 'macro_recall', 'macro_fscore'])\n",
    "\n",
    "\n",
    "class ClassifierQualityMetrics:\n",
    "\n",
    "\n",
    "    def __init__(self, truth, predictions, nclasses = 2):\n",
    "        \"\"\"Initialize class parameters\n",
    "\n",
    "        Args:\n",
    "           truth: ground truth\n",
    "           predictions: predicted labels\n",
    "           nclasses: number of classes\n",
    "        \"\"\"\n",
    "\n",
    "        self._truth = truth\n",
    "        self._predictions = predictions\n",
    "        if nclasses == 2:\n",
    "            self._computeTwoclassQualityMetrics()\n",
    "        elif nclasses > 2:\n",
    "            self._computeMulticlassQualityMetrics(nclasses)\n",
    "        else:\n",
    "            raise ValueError('nclasses must be at least 2')\n",
    "\n",
    "\n",
    "\n",
    "    def get(self, metric):\n",
    "        \"\"\"Get a metric from the quality metrics collection\n",
    "\n",
    "        Args:\n",
    "           metric: name of the metric to return\n",
    "\n",
    "        Returns:\n",
    "           A numeric value for the given metric\n",
    "        \"\"\"\n",
    "\n",
    "        return getattr(self._metrics, metric)\n",
    "\n",
    "\n",
    "\n",
    "    def _computeTwoclassQualityMetrics(self):\n",
    "        # Alg object for quality metrics computation\n",
    "        quality_alg = twoclass_quality.Batch()\n",
    "        # Get access to the input parameter\n",
    "        input = quality_alg.getInputDataCollection().getInput(\n",
    "                twoclass_quality.confusionMatrix)\n",
    "        # Pass ground truth and predictions as input\n",
    "        input.set(binary_confusion_matrix.groundTruthLabels, self._truth)\n",
    "        input.set(binary_confusion_matrix.predictedLabels, self._predictions)\n",
    "        # Compute confusion matrix\n",
    "        confusion = quality_alg.compute().getResult(twoclass_quality.confusionMatrix)\n",
    "        # Retrieve quality metrics from the confusion matrix\n",
    "        metrics = confusion.get(binary_confusion_matrix.binaryMetrics)\n",
    "        # Convert the metrics into a Python namedtuple and return it\n",
    "        block = BlockDescriptor_Float64()\n",
    "        metrics.getBlockOfRows(0, 1, readOnly, block)\n",
    "        x = block.getArray().flatten()\n",
    "        self._metrics = TwoClassMetrics(*x)\n",
    "        metrics.releaseBlockOfRows(block)\n",
    "\n",
    "\n",
    "\n",
    "    def _computeMulticlassQualityMetrics(self, nclasses):\n",
    "        # Alg object for quality metrics computation\n",
    "        quality_alg = multiclass_quality.Batch(nclasses)\n",
    "        # Get access to the input parameter\n",
    "        input = quality_alg.getInputDataCollection().getInput(\n",
    "                multiclass_quality.confusionMatrix)\n",
    "        # Pass ground truth and predictions as input\n",
    "        input.set(multiclass_confusion_matrix.groundTruthLabels, self._truth)\n",
    "        input.set(multiclass_confusion_matrix.predictedLabels, self._predictions)\n",
    "        # Compute confusion matrix\n",
    "        confusion = quality_alg.compute().getResult(multiclass_quality.confusionMatrix)\n",
    "        # Retrieve quality metrics from the confusion matrix\n",
    "        metrics = confusion.get(multiclass_confusion_matrix.multiClassMetrics)\n",
    "        # Convert the metrics into a Python namedtuple and return it\n",
    "        block = BlockDescriptor_Float64()\n",
    "        metrics.getBlockOfRows(0, 1, readOnly, block)\n",
    "        x = block.getArray().flatten()\n",
    "        self._metrics = MultiClassMetrics(*x)\n",
    "        metrics.releaseBlockOfRows(block)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many aspects in a multi-class classifier's quality metrics, but \"average accuracy\" is probably the most commonly used indicator in the model evaluation process. Utilize the `ClassifierQualityMetrics` class with right arguments to invoke the appropriate DAAL's model evaluation libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "quality = ClassifierQualityMetrics(test_labels, predictions, nclasses)\n",
    "print('Average accuracy: {:.2f}%'.format(quality.get('accuracy')*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize predictions\n",
    "The code below shows the last 10 images we did predictions for, together with the predicted labels. Does our classifier do a good job in guessing the labels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted = getArrayFromNT(predictions)\n",
    "images_and_labels = list(zip(digits.images[-10:], predicted[-10:]))\n",
    "for index, (image, label) in enumerate(images_and_labels):\n",
    "    plt.subplot(2, 5, index + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title('Prediction: %i' % label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare with scikit-learn SVC\n",
    "Finally, as a comparison, see a solution below that uses Support Vector Classifier from scikit-learn [`sklearn.svm.svc`](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html). We get the timings for both training and prediction. We also calculates the average accuracy of predictions. How does the PyDAAL solution compare against this solution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm, metrics\n",
    "\n",
    "sklearn_classifier = svm.SVC(kernel='linear')\n",
    "\n",
    "%time sklearn_classifier.fit(digits.data[:-100], digits.target[:-100])\n",
    "\n",
    "%time sklearn_predictions = sklearn_classifier.predict(digits.data[-100:])\n",
    "\n",
    "sklearn_quality = metrics.accuracy_score(digits.target[-100:], sklearn_predictions)\n",
    "print('Average accuracy: {:.2f}%'.format(sklearn_quality*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "In this lab we learned Support Vector Machine, a powerful classification algorithm. We saw how to use SVM in conjunction with a multi-class classifier to recognize handwritten digits. We also compared the execution time and the prediction quality between a PyDAAL solution and a scikit-learn solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
