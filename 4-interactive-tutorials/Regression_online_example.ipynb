{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Intel DAAL related imports\n",
    "from daal.data_management import (\n",
    "    DataSourceIface, FileDataSource, HomogenNumericTable, MergedNumericTable, NumericTable\n",
    ")\n",
    "\n",
    "from utils import printNumericTable\n",
    "from CustomUtils import getArrayFromNT\n",
    "\n",
    "# Import numpy, matplotlib, seaborn\n",
    "import numpy as np\n",
    "\n",
    "# Boilerplate\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting configurations\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online Ridge Regression \n",
    "\n",
    "### Tutorial brief\n",
    "This tutorial is an example of using ridge regression algorithms from pyDAAL to build predictive models.\n",
    "We use the well-studied Boston House Prices dataset to train ridge regression model in online processiong mode. We  test the accuracy of these models in median house price prediction. The code for ridge regression model training and prediction is provided.\n",
    "\n",
    "### Learning objectives\n",
    "* To understand how to process the data that doen not fit into memory using online computing mode. \n",
    "* To understand and practice the typical code sequence of using pyDAAL for supervised learning.\n",
    "* To practice interactions and conversions between DAAL NumericTables and NumPy ndarrays.\n",
    "\n",
    "\n",
    "### Linear regression introduction\n",
    "Supervised learning involves training a model using the data that has known responses, and then apply the model to predict responses for unseen data. In the case of **linear regression** and **ridge regression**, the model is linear. That is, \n",
    "\n",
    "$$ f_{\\beta}(X) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_k x_k $$\n",
    "\n",
    "$\\beta_0, \\beta_1, \\cdots, \\beta_k$ are the regression model coefficients. \n",
    "\n",
    "pyDAAL provides two linear regression algorithms:\n",
    "* **Multiple Linear Regression**: The model is trained by minimizing an objective function in the form of **Residual Sum of Squares**. pyDAAL supports two ways to train the model: 1) Normal Equation method, and 2) QR method.\n",
    "\n",
    "$$ \\sum \\limits_{i=1}^n\\left ( y_i - f_{\\beta}(X^i)\\right )^2 $$  \n",
    "* **Ridge Regression**: It is similar to multiple linear regression, but adds a regularization term to the objective function. The regularization term penalizes features with large values, thus makes the model less prone to overfitting. \n",
    "\n",
    "$$ \\sum \\limits_{i=1}^n\\left ( y_i - f_{\\beta}(X^i)\\right )^2 + \\lambda \\sum \\limits_{j=1}^k \\beta_j^2 $$\n",
    "\n",
    "### Online processing mode\n",
    "Some Intel DAAL algorithms enable processing of data sets in blocks. In the online processing mode, the `compute()`, and `finalizeCompute()` methods of a particular algorithm class are used.\n",
    "\n",
    "This computation mode assumes that the data arrives in blocks i = 1, 2, 3, â€¦ nBlocks.\n",
    "\n",
    "Call the `compute()` method each time new input becomes available.\n",
    "![](https://software.intel.com/sites/products/documentation/doclib/daal/daal-user-and-reference-guides/daal_prog_guide/GUID-73A24EF1-070A-40DA-A3A9-FD62079C370A-low.png)\n",
    "\n",
    "When the last block of data arrives, call the `finalizeCompute()` method to produce final results.\n",
    "![](https://software.intel.com/sites/products/documentation/doclib/daal/daal-user-and-reference-guides/daal_prog_guide/GUID-65441AC7-7991-4966-98D4-FD49E3313889-low.png)\n",
    "\n",
    "If the input data arrives in an asynchronous mode, you can use the `getStatus()` method for a given data source to check whether a new block of data is available for load."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Boston House Prices dataset\n",
    "The dataset has already been downloaded to the ./mldata folder. There are 506 rows and 14 columns. The first 13 columns are features (explanatory variables), and the last column is the dependent variable we try to make predictions for. Here's detailed information about this dataset, including descriptions of each feature:\n",
    "\n",
    "> Origin: \n",
    "\n",
    "> This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University. \n",
    "\n",
    "> Creator: \n",
    "\n",
    "> Harrison, D. and Rubinfeld, D.L. \n",
    "> 'Hedonic prices and the demand for clean air', J. Environ. Economics & Management, vol.5, 81-102, 1978.\n",
    "\n",
    "> Data Set Information:\n",
    "\n",
    "> Concerns housing values in suburbs of Boston.\n",
    "\n",
    "\n",
    "> Attribute Information:\n",
    "\n",
    "> 1. CRIM: per capita crime rate by town \n",
    "> 2. ZN: proportion of residential land zoned for lots over 25,000 sq.ft. \n",
    "> 3. INDUS: proportion of non-retail business acres per town \n",
    "> 4. CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) \n",
    "> 5. NOX: nitric oxides concentration (parts per 10 million) \n",
    "> 6. RM: average number of rooms per dwelling \n",
    "> 7. AGE: proportion of owner-occupied units built prior to 1940 \n",
    "> 8. DIS: weighted distances to five Boston employment centres \n",
    "> 9. RAD: index of accessibility to radial highways \n",
    "> 10. TAX: full-value property-tax rate per \\$10,000 \n",
    "> 11. PTRATIO: pupil-teacher ratio by town \n",
    "> 12. B: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town \n",
    "> 13. LSTAT: % lower status of the population \n",
    "> 14. MEDV: Median value of owner-occupied homes in \\$1000's\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mse(values, fitted_values):\n",
    "    \"\"\"Return Mean Squared Errors for fitted values w.r.t. true values\n",
    "\n",
    "    Args:\n",
    "        values: True values. NumericTable, nsamples-by-noutputs\n",
    "        fitted_values: True values. NumericTable, nsamples-by-noutputs\n",
    "\n",
    "    Returns:\n",
    "        A tuple contains MSE's\n",
    "    \"\"\"\n",
    "\n",
    "    y_t = getArrayFromNT(values)\n",
    "    y_p = getArrayFromNT(fitted_values)\n",
    "    rss = ((y_t - y_p) ** 2).sum(axis = 0)\n",
    "    mse = rss / y_t.shape[0]\n",
    "    return tuple(mse)\n",
    "\n",
    "\n",
    "def score(y_true, y_pred):\n",
    "    \"\"\"Compute R-squared and adjusted R-squared\n",
    "\n",
    "    Args:\n",
    "        y_true: True values. NumericTable, shape = (nsamples, noutputs)\n",
    "        y_pred: Predicted values. NumericTable, shape = (nsamples, noutputs)\n",
    "\n",
    "    Returns:\n",
    "        R2: A tuple with noutputs values\n",
    "    \"\"\"\n",
    "\n",
    "    y_t = getArrayFromNT(y_true)\n",
    "    y_p = getArrayFromNT(y_pred)\n",
    "    rss = ((y_t - y_p) ** 2).sum(axis = 0)\n",
    "    tss = ((y_t - y_t.mean(axis = 0)) ** 2).sum(axis = 0)\n",
    "    r2 = 1 - rss/tss\n",
    "    return tuple(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regression model training for Boston houses prices\n",
    "The code below reads data from file `housing.data.train.csv` and creates 2 NumericTables: training data (`xTrain`) and ground truth (`yTrain`). We use the `FileDataSource` to stream the data from the file into in-memory representation - numeric tables. \n",
    "\n",
    "The model of ridge regression gets and update after each new block of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from daal.algorithms.ridge_regression import training as ridge_training\n",
    "\n",
    "# Number of teatures in the dataset\n",
    "nFeatures = 13\n",
    "\n",
    "# Initialize FileDataSource to retrieve the input data from a .csv file\n",
    "trainDataSource = FileDataSource(\n",
    "    './mldata/housing.data.train.csv', DataSourceIface.notAllocateNumericTable, DataSourceIface.doDictionaryFromContext\n",
    ")\n",
    "\n",
    "# Create Numeric Tables for training data and dependent variables\n",
    "xTrain = HomogenNumericTable(nFeatures, 0, NumericTable.notAllocate)\n",
    "yTrain = HomogenNumericTable(1,         0, NumericTable.notAllocate)\n",
    "mergedDataTrain = MergedNumericTable(xTrain, yTrain)\n",
    "\n",
    "# Create an algorithm object to train ridge regression model in online processing mode\n",
    "regr = ridge_training.Online()\n",
    "\n",
    "while(trainDataSource.loadDataBlock(50, mergedDataTrain) == 50):\n",
    "    # Pass new block of data from the training data set and dependent values to the algorithm\n",
    "    regr.input.set(ridge_training.data, xTrain)\n",
    "    regr.input.set(ridge_training.dependentVariables, yTrain)\n",
    "\n",
    "    # Update ridge regression model\n",
    "    regr.compute()\n",
    "\n",
    "model = regr.finalizeCompute().get(ridge_training.model)\n",
    "\n",
    "# Peek at the model (Betas)\n",
    "printNumericTable(model.getBeta())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction with Ridge Regression model\n",
    "\n",
    "The code below reads data from file housing.data.test.csv and creates 2 NumericTables: test data (xTest) and test ground truth (yTest). We use ridge regression prediction algorithm and the model obtained on the training stage to compute the predictions for a new, prevoiusly unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from daal.algorithms.ridge_regression import prediction as ridge_prediction\n",
    "\n",
    "testDataSource = FileDataSource(\n",
    "    './mldata/housing.data.test.csv', DataSourceIface.notAllocateNumericTable, DataSourceIface.doDictionaryFromContext\n",
    ")\n",
    "\n",
    "# Create Numeric Tables for testing data and dependent variables\n",
    "xTest = HomogenNumericTable(nFeatures, 0, NumericTable.notAllocate)\n",
    "yTest = HomogenNumericTable(1,         0, NumericTable.notAllocate)\n",
    "mergedDataTest = MergedNumericTable(xTest, yTest)\n",
    "\n",
    "testDataSource.loadDataBlock(mergedDataTest)\n",
    "\n",
    "# Create a prediction algorithm object\n",
    "alg = ridge_prediction.Batch()\n",
    "# Set input\n",
    "alg.input.setModel(ridge_prediction.model, model)\n",
    "alg.input.setTable(ridge_prediction.data, xTest)\n",
    "\n",
    "# Compute\n",
    "predictions = alg.compute().get(ridge_prediction.prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting predicted values against the ground truth\n",
    "To see if the model has done a good job, we plot the predicted values against the ground truth. If the model does a perfect job then all points on the plot should fall on a straight line. As we see, it's not quite the case. But still the predictions are close to true values in many cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(mse(yTest, predictions))\n",
    "print(score(yTest, predictions))\n",
    "\n",
    "predicted = getArrayFromNT(predictions)\n",
    "expected = getArrayFromNT(yTest)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(expected, predicted)\n",
    "ax.plot([0, 30], [0, 30], 'k--', lw=4)\n",
    "ax.set_xlabel('Measured')\n",
    "ax.set_ylabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "In this lab, we learned two widely used linear regression models: Multiple linear regression and Ridge regression. We saw how to apply them to the Boston House Prices dataset. We studied and practiced pyDAAL API for these two algorithms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
