{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Boilerplate\n",
    "%matplotlib inline\n",
    "\n",
    "# Intel DAAL related imports\n",
    "from daal.data_management import HomogenNumericTable\n",
    "\n",
    "\n",
    "from CustomUtils import getArrayFromNT\n",
    "\n",
    "# Import numpy, matplotlib, seaborn\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting configurations\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering with K-Means\n",
    "\n",
    "### Tutorial brief \n",
    "This tutorial consists of two parts. Firstly, we walk through an example of using the K-Means form pyDAAL for clustering the Iris dataset. Secondly, we try K-Means for a dataset with a much higher dimensionality (90 features). We reduce the dimensionality to 2 before clustering, such that the clusters can be easily visualized. We use PCA for dimensionality reduction. The K-Means code is provided. The PCA code is left as an exercise. \n",
    "\n",
    "### Learning objectives \n",
    "* To get familiar with pyDAAL API look-and-feel.\n",
    "* To understand and practice the typical code sequence of using pyDAAL for unsupervised learning.\n",
    "\n",
    "### K-Means introduction \n",
    "K-Means is a commonly used clustering algorithm. It takes input data and a given number of clusters $k$, then iteratively computes $k$ centroids for $k$ clusters, and assigns observations in the input dataset to these clusters. The algorithm minimizes the within-cluster sum of distances between observations and cluster centroids. This is called the goal function:\n",
    "$$\\sum\\limits_{i=1}^{k} \\sum\\limits_{j:x_j\\in S_i}^n\\|x_j-m_i\\|^2$$\n",
    "Here, $k$ is the number of clusters, $n$ is the number of observations, $S_i$ is the $i$-th cluster with $m_i$ as its centroid, $x_j$ is the $j$-th observation.\n",
    "\n",
    "### PCA introduction \n",
    "Principal Component Analysis (PCA) is a commonly used technique for dimensionality reduction. The algorithm computes $p$ eigenvalues and $p$ eigenvectors from the correlation matrix of the input data, where $p$ is the number of features. Multiplying the largest $d$ eigenvectors (according to the eigenvalues) to the input data gives a reduced dataset with $d$ dimensions ($d < p$). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Iris dataset \n",
    "We use the Iris dataset from `sklearn.datasets`. The code below loads the data and displays some basic information of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "print('Shape:', iris.data.shape)\n",
    "print('Features:', iris.feature_names)\n",
    "print('Labels: ', iris.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means code using pyDAAL \n",
    "The code below defines `class KMeans`, which encapsulates the K-Means algorithm from pyDAAL. The implementation is in the `compute` method. Note that there are 2 steps. The initialization step is to randomly pick $k$ centroids. The clustering step runs up to `maxiter` iterations: Each iteration assign each of all observations to the closest centroid, and then re-computes the centroids. The code returns 4 pieces of results: `centroids`, `assignments`, `goalFunction`, and `niterations`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import daal.algorithms.kmeans as kmeans\n",
    "from daal.algorithms.kmeans import init\n",
    "\n",
    "\n",
    "class KMeans:\n",
    "\n",
    "    def __init__(self, nclusters, randomseed = None):\n",
    "        \"\"\"Initialize class parameters\n",
    "        \n",
    "        Args:\n",
    "           nclusters: Number of clusters\n",
    "           randomseed: An integer used to seed the random number generator\n",
    "        \"\"\"\n",
    "\n",
    "        self.nclusters_ = nclusters\n",
    "        self.seed_ = 1234 if randomseed is None else randomseed\n",
    "        self.centroids_ = None\n",
    "        self.assignments_ = None\n",
    "        self.goalfunction_ = None\n",
    "        self.niterations_ = None\n",
    "\n",
    "\n",
    "    def compute(self, data, centroids = None, maxiters = 100):\n",
    "        \"\"\"Compute K-Means clustering for the input data\n",
    "\n",
    "        Args:\n",
    "           data: Input data to be clustered\n",
    "           centroids: User defined input centroids. If None then initial\n",
    "               centroids will be randomly chosen\n",
    "           maxiters: The maximum number of iterations\n",
    "        \"\"\"\n",
    "\n",
    "        if centroids is None:\n",
    "            # Create an algorithm object for centroids initialization\n",
    "            init_alg = init.Batch_Float64RandomDense(self.nclusters_)\n",
    "            # Set input\n",
    "            init_alg.input.set(init.data, data)\n",
    "            # Set parameters\n",
    "            init_alg.parameter.seed = self.seed_\n",
    "            # Compute initial centroids\n",
    "            self.centroids_ = init_alg.compute().get(init.centroids)\n",
    "        else:\n",
    "            self.centroids_ = centroids\n",
    "\n",
    "        # Create an algorithm object for clustering\n",
    "        clustering_alg = kmeans.Batch_Float64LloydDense(\n",
    "                self.nclusters_,\n",
    "                maxiters)\n",
    "        # Set input\n",
    "        clustering_alg.input.set(kmeans.data, data)\n",
    "        clustering_alg.input.set(kmeans.inputCentroids, self.centroids_)\n",
    "        # compute\n",
    "        result = clustering_alg.compute()\n",
    "        self.centroids_ = result.get(kmeans.centroids)\n",
    "        self.assignments_ = result.get(kmeans.assignments)\n",
    "        self.goalfunction_ = result.get(kmeans.goalFunction)\n",
    "        self.niterations_ = result.get(kmeans.nIterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering and visualization \n",
    "We now apply K-Means to the Iris dataset. A few things to note:\n",
    "* The original input data is an ndarray. We need to convert it to a DAAL NumericTable. \n",
    "* The number of clusters is set to 3 because there're 3 types of Iris in the data. \n",
    "* We want to visualize the clustering, so we get `assignments` from the results, which is a NumericTable. We convert it back to an ndarray so it can be passed into the plotting function.\n",
    "* Although there are 4 dimensions in the data (see `iris.feature_names` above), we arbitrarily choose 3 ('Petal width', 'Sepal length', and 'Petal length') to create a 3D plot.\n",
    "* On the 3D plot, data points are colored according to their assigned labels. This allows us to see the clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a NumericTable from the Iris dataframe\n",
    "iris_data = HomogenNumericTable(iris.data.astype(dtype=np.double))\n",
    "\n",
    "# The number of clusters is 3, as there're 3 labels\n",
    "nclusters = len(np.unique(iris.target))\n",
    "\n",
    "# K-Means clustering\n",
    "clustering = KMeans(nclusters)\n",
    "clustering.compute(iris_data)\n",
    "assignments = getArrayFromNT(clustering.assignments_).flatten().astype(np.int)\n",
    "\n",
    "# Visualize 3 clusters using a 3D plot\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "plt.set_cmap(plt.cm.prism)\n",
    "plt.clf()\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection = '3d')\n",
    "ax.scatter(iris.data[:, 3], iris.data[:, 0], iris.data[:, 2], c=assignments)\n",
    "ax.w_xaxis.set_ticklabels([])\n",
    "ax.w_yaxis.set_ticklabels([])\n",
    "ax.w_zaxis.set_ticklabels([])\n",
    "ax.set_xlabel('Petal width')\n",
    "ax.set_ylabel('Sepal length')\n",
    "ax.set_zlabel('Petal length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Clustering for a high-dimensional dataset\n",
    "In this section we take a look at the [Libras Movement Dataset](http://archive.ics.uci.edu/ml/datasets/Libras+Movement) from the UCI machine learning repository.\n",
    "\n",
    "The data has been pre-downloaded in the './mldata' folder, and can be loaded into memory using `np.genfromtxt(\"./mldata/movement_libras.data\", ...)`.\n",
    "\n",
    "According to the dataset description:\n",
    ">--- LIBRAS, acronym of the Portuguese name \"LIngua BRAsileira de Sinais\", is the oficial brazilian sign language.\n",
    "\n",
    ">--- The dataset (movement_libras) contains 15 classes of 24 instances each, where each class references to a hand \n",
    ">       movement type in LIBRAS. The hand movement is represented as a bidimensional curve performed by the hand in a \n",
    ">       period of time. The curves were obtained from videos of hand movements, with the Libras performance from 4 \n",
    ">       different people, during 2 sessions. Each video corresponds to only one hand movement and has about 7 seconds. \n",
    "\n",
    "The number of features of the data is 90, corresponding to columns 0 through 89 in the input. The last column (90) is the class labels. For cluster analysis, we could have applied K-Means the same way we did in the previous section. However, it will be very difficult, if not impossible, to visualize it. In this section, you are asked to implement a PCA class using pyDAAL. After that, we use the the PCA to reduce the dimensionality to 2. We apply K-Means to the reduced dataset. And then we can visualize the clustering with a 2D plot.\n",
    "\n",
    "The skeleton of the PCA class code is given below. Please provide an implementation for the **`compute`** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import daal.algorithms.pca as pca\n",
    "\n",
    "class PCA:\n",
    "\n",
    "    def __init__(self, method = 'correlation'):\n",
    "        \"\"\"Initialize class parameters\n",
    "\n",
    "        Args:\n",
    "           method: The default method is based on correation matrix. It\n",
    "           can also be the SVD method ('svd')\n",
    "        \"\"\"\n",
    "\n",
    "        if method != 'correlation' and method != 'svd':\n",
    "            warnings.warn(method + \n",
    "            ' method is not supported. Default method is used', \n",
    "            UserWarning)\n",
    "\n",
    "        self.method_ = method\n",
    "        self.eigenvalues_ = None\n",
    "        self.eigenvectors_ = None\n",
    "\n",
    "\n",
    "    def compute(self, data):\n",
    "        \"\"\"Compute PCA the input data\n",
    "\n",
    "        Args:\n",
    "           data: Input data \n",
    "        \"\"\"\n",
    "               \n",
    "        # Create an algorithm object for PCA\n",
    "        #\n",
    "        # YOUR CODE HERE\n",
    "        # The algorithm class you need is either Batch_Float64SvdDense or Batch_Float64CorrelationDense.\n",
    "      \n",
    "\n",
    "        # Set input\n",
    "        #\n",
    "        # YOUR CODE HERE\n",
    "        # Use the 'input.setDataSet' member method of the algorithm class to set input. \n",
    "       \n",
    "        # Th signature of the method is: input.setDataset(InputID, input)\n",
    "        # You should use 'pca.data' for InputID.\n",
    "        \n",
    "        # compute\n",
    "        #\n",
    "        # YOUR CODE HERE\n",
    "        # You should store the return value of compute to 'result'\n",
    "        \n",
    "        \n",
    "        self.eigenvalues_ = result.get(pca.eigenvalues)\n",
    "        self.eigenvectors_ = result.get(pca.eigenvectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the PCA class is implemented. We can use it for dimensionality reduction. The `eigenvectors` from PCA result contains $p$ eigenvectors in the row major order and sorted from the largest to the smallest. In this case, we take the first (largest) two eigenvectors and multiply them with the original data using NumPy's `dot` function. This gives us a reduced dataset with 2 columns. The number of rows doesn't change.\n",
    "\n",
    "Next, apply pyDAAL KMeans on the reduced data. Assignments (labels) and the final centroids are retrieved from the result for visualization (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from pca import *\n",
    "\n",
    "# Load data from the CSV file\n",
    "tra = np.genfromtxt('./mldata/movement_libras.data', dtype = np.double, delimiter=',', usecols=list(range(90)))\n",
    "\n",
    "# Create a NumericTable\n",
    "tra_data = HomogenNumericTable(tra)\n",
    "\n",
    "# Dimensionality reduction\n",
    "pr = PCA(method='svd')\n",
    "pr.compute(tra_data)\n",
    "loadings = getArrayFromNT(pr.eigenvectors_)\n",
    "\n",
    "# Multiply the largest 2 eigenvectors with the original data\n",
    "tra_reduced = np.dot(tra, loadings[:2].T)\n",
    "tra_reduced_data = HomogenNumericTable(tra_reduced)\n",
    "\n",
    "# K-Means clustering on the reduced dataset\n",
    "nclusters = 15\n",
    "clustering = KMeans(nclusters)\n",
    "clustering.compute(tra_reduced_data, maxiters=1000)\n",
    "assignments = getArrayFromNT(clustering.assignments_).flatten().astype(np.int)\n",
    "centroids = getArrayFromNT(clustering.centroids_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below creates a 2D plot to show the 10 clusters. Each sample is colored according to the assigned label. The centroids of the clusters are shown as big black x's on the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.set_cmap(plt.cm.Dark2)\n",
    "plt.clf()\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(tra_reduced[:, 0], tra_reduced[:, 1], s=50, c=assignments)\n",
    "ax.scatter(centroids[:, 0], centroids[:, 1], marker='x', s=180, linewidths=4, color='black')\n",
    "ax.xaxis.set_ticklabels([])\n",
    "ax.yaxis.set_ticklabels([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "In this lab, we learned two unsupervised learning algorithms: K-Means clustering and PCA. We saw how to apply them to cluster and visualize the handwritten digits dataset. We studied and practiced pyDAAL API for these two algorithms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
