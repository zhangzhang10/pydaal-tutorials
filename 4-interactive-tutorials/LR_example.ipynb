{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Boilerplate\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from utils import printNumericTable\n",
    "from CustomUtils import getArrayFromNT\n",
    "\n",
    "\n",
    "# Import numpy, matplotlib, seaborn\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting configurations\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 9)\n",
    "\n",
    "# Intel DAAL related imports\n",
    "from daal.algorithms.linear_regression import training as lr_training\n",
    "from daal.algorithms.linear_regression import prediction as lr_prediction\n",
    "from daal.data_management import HomogenNumericTable\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression \n",
    "\n",
    "### Tutorial brief\n",
    "This tutorial is an example of using linear regression algorithms from pyDAAL to build predictive models. We use the well-studied Boston House Prices dataset to train a basic multiple linear regression model, and then a Ridge regression model. We  test the performance of these models in median house price prediction. The code for multiple linear regression model training and prediction is provided. You are required to write the code for Ridge regression model training and prediction.\n",
    "\n",
    "### Learning objectives\n",
    "* To understand and practice the typical code sequence of using pyDAAL for supervised learning.\n",
    "* To practice interactions and conversions between DAAL NumericTables and NumPy ndarrays.\n",
    "\n",
    "### Linear regression introduction\n",
    "Supervised learning involves training a model using data that has known responses, and then apply the model to predict responses for unseen data. In the case of **linear regression**, the model is linear. That is, \n",
    "\n",
    "$$ f_{\\beta}(X) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_k x_k $$\n",
    "\n",
    "$\\beta_0, \\beta_1, \\cdots, \\beta_k$ are the regression model coefficients. \n",
    "\n",
    "pyDAAL provides two linear regression algorithms:\n",
    "* **Multiple Linear Regression**: The model is trained by minimizing an objective function in the form of **Residual Sum of Squares**. pyDAAL supports two ways to train the model: 1) Normal Equation method, and 2) QR method.\n",
    "\n",
    "$$ \\sum \\limits_{i=1}^n\\left ( y_i - f_{\\beta}(X^i)\\right )^2 $$  \n",
    "* **Ridge Regression**: It is similar to multiple linear regression, but adds a regularization term to the objective function. The regularization term penalizes features with large values, thus makes the model less prone to overfitting. \n",
    "\n",
    "$$ \\sum \\limits_{i=1}^n\\left ( y_i - f_{\\beta}(X^i)\\right )^2 + \\alpha \\sum \\limits_{j=1}^k \\beta_j^2 $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Boston House Prices dataset\n",
    "The dataset has already been downloaded to the ./mldata folder. There are 506 rows and 14 columns. The first 13 columns are features (explanatory variables), and the last column is the dependent variable we try to make predictions for. Here's detailed information about this dataset, including descriptions of each feature:\n",
    "\n",
    "> Origin: \n",
    "\n",
    "> This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University. \n",
    "\n",
    "> Creator: \n",
    "\n",
    "> Harrison, D. and Rubinfeld, D.L. \n",
    "> 'Hedonic prices and the demand for clean air', J. Environ. Economics & Management, vol.5, 81-102, 1978.\n",
    "\n",
    "> Data Set Information:\n",
    "\n",
    "> Concerns housing values in suburbs of Boston.\n",
    "\n",
    "\n",
    "> Attribute Information:\n",
    "\n",
    "> 1. CRIM: per capita crime rate by town \n",
    "> 2. ZN: proportion of residential land zoned for lots over 25,000 sq.ft. \n",
    "> 3. INDUS: proportion of non-retail business acres per town \n",
    "> 4. CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) \n",
    "> 5. NOX: nitric oxides concentration (parts per 10 million) \n",
    "> 6. RM: average number of rooms per dwelling \n",
    "> 7. AGE: proportion of owner-occupied units built prior to 1940 \n",
    "> 8. DIS: weighted distances to five Boston employment centres \n",
    "> 9. RAD: index of accessibility to radial highways \n",
    "> 10. TAX: full-value property-tax rate per \\$10,000 \n",
    "> 11. PTRATIO: pupil-teacher ratio by town \n",
    "> 12. B: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town \n",
    "> 13. LSTAT: % lower status of the population \n",
    "> 14. MEDV: Median value of owner-occupied homes in \\$1000's\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection\n",
    "To fit a good linear model, we usually need to carefully decide which features to be included as explanatory variables. Features that are not statistically significant should be excluded. Features that are highly correlated among themselves should also be excluded. Feature selection is an important topic by itself. But it is beyond the scope of this tutorial. For our purposes, let's just say we exclude `indus`, `age`, `zn`, `tax`, and `rad`. This leaves `crim`, `chas`, `nox`, `rm`, `dis`, `ptratio`, `b`, and `lstat` as the explanatory variables. These are corresponding to column indices 0, 3, 4, 5, 7, 10, 11, and 12 in the data. Later, when loading data into memory we'll choose only these columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple linear regression code using pyDAAL\n",
    "The code below defines `class LinearRegression`, which encapsulates the basic multiple linear regression algorithm from pyDAAL. An instance of the class can be initialized using `normEq` or `qr`, corresponding to the two model training methods. Method `train` takes training data and the known response to learn a model and return it. Method `predict` takes a model object and test data, and make predictions. The code also defines function `getBetas` that returns the regression coefficients of a model; function `mse` that returns _mean squared errors_ (MSE) for given true values and fitted values; function `scores` that computes coefficient of determination _R-squared_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from daal.algorithms.linear_regression import training as lr_training\n",
    "from daal.algorithms.linear_regression import prediction as lr_prediction\n",
    "\n",
    "def getBetas(linear_model):\n",
    "    \"\"\"Return regression coefficients for a given linear model\n",
    "\n",
    "    Args:\n",
    "        linear_model: A trained model\n",
    "\n",
    "    Returns:\n",
    "        A n-by-(k+1) NumericTable contains betas, where n is the number of dependent\n",
    "        variables; k is the number of features (independent variables)\n",
    "    \"\"\"\n",
    "\n",
    "    return linear_model.getBeta()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def mse(values, fitted_values):\n",
    "    \"\"\"Return Mean Squared Errors for fitted values w.r.t. true values\n",
    "\n",
    "    Args:\n",
    "        values: True values. NumericTable, nsamples-by-noutputs\n",
    "        fitted_values: True values. NumericTable, nsamples-by-noutputs\n",
    "\n",
    "    Returns:\n",
    "        A tuple contains MSE's\n",
    "    \"\"\"\n",
    "\n",
    "    y_t = getArrayFromNT(values)\n",
    "    y_p = getArrayFromNT(fitted_values)\n",
    "    rss = ((y_t - y_p) ** 2).sum(axis = 0)\n",
    "    mse = rss / y_t.shape[0]\n",
    "    return tuple(mse)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def score(y_true, y_pred):\n",
    "    \"\"\"Compute R-squared and adjusted R-squared\n",
    "\n",
    "    Args:\n",
    "        y_true: True values. NumericTable, shape = (nsamples, noutputs)\n",
    "        y_pred: Predicted values. NumericTable, shape = (nsamples, noutputs)\n",
    "\n",
    "    Returns:\n",
    "        R2: A tuple with noutputs values\n",
    "    \"\"\"\n",
    "\n",
    "    y_t = getArrayFromNT(y_true)\n",
    "    y_p = getArrayFromNT(y_pred)\n",
    "    rss = ((y_t - y_p) ** 2).sum(axis = 0)\n",
    "    tss = ((y_t - y_t.mean(axis = 0)) ** 2).sum(axis = 0)\n",
    "    r2 = 1 - rss/tss\n",
    "    return tuple(r2)\n",
    "\n",
    "\n",
    "class LinearRegression:\n",
    "\n",
    "\n",
    "    def __init__(self, method = 'normEq'):\n",
    "        \"\"\"Initialize class parameters\n",
    "\n",
    "        Args:\n",
    "           method: The default method is based on Normal Equation ('normEq'). It\n",
    "           can also be QR method ('qr')\n",
    "        \"\"\"\n",
    "\n",
    "        if method != 'normEq' and method != 'qr':\n",
    "            warnings.warn(method + \n",
    "            ' method is not supported. Default method is used', \n",
    "            UserWarning)\n",
    "\n",
    "        self.method_ = method\n",
    "\n",
    "\n",
    "\n",
    "    def train(self, data, responses):\n",
    "        \"\"\"Train a Linear Regression model.\n",
    "\n",
    "        Args:\n",
    "            data: Training data\n",
    "            responses: Known responses to the training data\n",
    "\n",
    "        Returns:\n",
    "            A Linear Regression model object\n",
    "        \"\"\"\n",
    "\n",
    "        # Create a training algorithm object\n",
    "        if self.method_ == 'qr': \n",
    "            lr_training_alg = lr_training.Batch(method=lr_training.qrDense)\n",
    "        else:\n",
    "            lr_training_alg = lr_training.Batch(method=lr_training.normEqDense)\n",
    "        # Set input\n",
    "        lr_training_alg.input.set(lr_training.data, data)\n",
    "        lr_training_alg.input.set(lr_training.dependentVariables, responses)\n",
    "        # Compute\n",
    "        results = lr_training_alg.compute()\n",
    "        # Return the trained model\n",
    "        return results.get(lr_training.model)\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, model, testdata):\n",
    "        \"\"\"Make prediction for unseen data using a trained model\n",
    "\n",
    "        Args:\n",
    "            model: A trained model\n",
    "            testdata: New data\n",
    "            intercept: A boolean to inidicate if intercept needs to be computed \n",
    "\n",
    "        Returns:\n",
    "            A NumericTable containing predicted responses \n",
    "        \"\"\"\n",
    "\n",
    "        # Create a prediction algorithm object\n",
    "        lr_prediction_alg = lr_prediction.Batch()\n",
    "        # Set input\n",
    "        lr_prediction_alg.input.setModel(lr_prediction.model, model)\n",
    "        lr_prediction_alg.input.setTable(lr_prediction.data, testdata)\n",
    "        # Set parameters\n",
    "        # Compute\n",
    "        results = lr_prediction_alg.compute()\n",
    "        return results.get(lr_prediction.prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression model training and prediction for Boston houses prices\n",
    "We save the last 50 samples from the entire dataset as test data, and use the rest as training data. The code below uses the `LinearRegression` class defined above. After training a model, we use it to predict prices for the last 50 samples. The code also shows computation of R-squared using the `scores` function, and retrieve the regression coefficients (betas) from the model.\n",
    "\n",
    "The code below reads data from file `housing.data` and creates 4 NumericTables: training data (`boston_X_train`), training data ground truth (`boston_Y_train`), test data (`boston_X_test`), and test data ground truth (`boston_Y_test`). We use the `numpy.genfromtxt` function to read data from the file into ndarrays. Then, create NumericTables from the ndarrays. _Note that when creating NumericTables from ndarrays, it is critical the ndarrays must be C-contiguous in memory._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use only `crim`, `chas`, `nox`, `rm`, `dis`, `ptratio`, `b`, and `lstat` for explanatory variables\n",
    "cols = (0,3,4,5,7,10,11,12)\n",
    "import numpy as np\n",
    "\n",
    "data = np.genfromtxt('./mldata/housing.data', dtype=np.double, usecols=cols)\n",
    "truth = np.genfromtxt('./mldata/housing.data', dtype=np.double, usecols=(13))\n",
    "\n",
    "# Training data and responses\n",
    "boston_X_train = HomogenNumericTable(data[:-50])\n",
    "boston_Y_train = HomogenNumericTable(truth[:-50].reshape(-1,1))\n",
    "\n",
    "# Test data and ground truth\n",
    "boston_X_test = HomogenNumericTable(data[-50:])\n",
    "boston_Y_test = HomogenNumericTable(truth[-50:].reshape(-1,1))\n",
    "\n",
    "# Use the LinearRegression class for model training and prediction\n",
    "regr = LinearRegression()\n",
    "model = regr.train(boston_X_train, boston_Y_train)\n",
    "predictions = regr.predict(model, boston_X_test)\n",
    "\n",
    "# Calculate the coefficient of determination\n",
    "R2 = score(boston_Y_test, predictions)\n",
    "print(R2)\n",
    "\n",
    "# Peek at the model (Betas)\n",
    "printNumericTable(getBetas(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting predicted values agains the ground truth\n",
    "To see if the model has done a good job, we plot the predicted values against the ground truth. If the model does a perfect job then all points on the plot should fall on a straight line. As we see, it's not quite the case. But still the predictions are close to true values in many cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = truth[-50:]\n",
    "predicted = getArrayFromNT(predictions)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(y,predicted)\n",
    "ax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\n",
    "ax.set_xlabel('Measured')\n",
    "ax.set_ylabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Implementing Ridge regression\n",
    "In this exercise, you are asked to implement a class for Ridge regression using pyDAAL. Ridge regression is basically linear regression with regularizations. The skeleton of the Ridge class code is given below. It has two methods, **train** and **predict**. The implementation for **train** is already provided. You only need to follow it as a recipe to implement the **predict** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from daal.algorithms.ridge_regression import training as ridge_training\n",
    "from daal.algorithms.ridge_regression import prediction as ridge_prediction\n",
    "\n",
    "class Ridge:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "    def train(self, data, responses, alpha = 1.0):\n",
    "        \"\"\"Train a Ridge Regression model.\n",
    "\n",
    "        Args:\n",
    "           data: Training data\n",
    "           responses: Known responses to the training data\n",
    "           alpha: Regularization parameter, a small positive value with default\n",
    "           1.0\n",
    "\n",
    "        Returns:\n",
    "            A Ridge Regression model object\n",
    "        \"\"\"\n",
    "\n",
    "        # Create a training algorithm object\n",
    "        ridge_training_alg = ridge_training.Batch() \n",
    "        # Set input\n",
    "        ridge_training_alg.input.set(ridge_training.data, data)\n",
    "        ridge_training_alg.input.set(ridge_training.dependentVariables, responses)\n",
    "        # Set parameter\n",
    "        alpha_nt = HomogenNumericTable(np.array([alpha], ndmin=2))\n",
    "        ridge_training_alg.parameter.ridgeParameters = alpha_nt\n",
    "        ridge_training_alg.parameter.interceptFlag = True\n",
    "        # Compute\n",
    "        results = ridge_training_alg.compute()\n",
    "        # Return the trained model\n",
    "        return results.get(ridge_training.model)\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, model, testdata, intercept = True):\n",
    "        \"\"\"Make prediction for unseen data using a trained model\n",
    "\n",
    "        Args:\n",
    "            model: A trained model\n",
    "            testdata: New data\n",
    "            intercept: A boolean to inidicate if intercept needs to be computed \n",
    "\n",
    "        Returns:\n",
    "            A NumericTable containing predicted responses \n",
    "        \"\"\"\n",
    "\n",
    "        # Create a prediction algorithm object\n",
    "        #\n",
    "        # YOUR CODE HERE\n",
    "        #\n",
    "        # The algorithm class you want to use is ridge_prediction.Batch\n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "        # Set input\n",
    "        #\n",
    "        # YOUR CODE HERE\n",
    "        #\n",
    "        # There are two pieces of input to be set: a pre-trained model and input data. You should\n",
    "        # use the 'input.setModelInput' and the 'input.setNumericTableInput' member methods of the\n",
    "        # algorithm object. The input IDs to use are 'ridge_prediction.model' and 'ridge_prediction.data'\n",
    "        # respectively.\n",
    "        \n",
    "\n",
    "        # Set parameters\n",
    "        #\n",
    "        # YOUR CODE HERE\n",
    "        #\n",
    "        # There is only one parameter we need to set. 'parameter.interceptFlag', it is a member variable \n",
    "        # of the algorithm object. \n",
    "        \n",
    "     \n",
    "        # Compute\n",
    "        #\n",
    "        # YOUR CODE HERE\n",
    "        #\n",
    "        # Call the 'compute' method of your algorithm object, and store the result to 'results'.\n",
    "        \n",
    "        return results.get(ridge_prediction.prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your Ridge regression class using the same Boston House Prices dataset. Note that all data read in previous steps is still available. You can try different values of $\\alpha$ (regularization parameter) to see its effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from regression import *\n",
    "\n",
    "# Use the Ridge class for model training and prediction\n",
    "regr = Ridge()\n",
    "model = regr.train(boston_X_train, boston_Y_train, alpha = 1.0)\n",
    "predictions = regr.predict(model, boston_X_test)\n",
    "\n",
    "# Calculate the coefficient of determination\n",
    "R2 = score(boston_Y_test, predictions)\n",
    "\n",
    "\n",
    "y = truth[-50:]\n",
    "predicted = getArrayFromNT(predictions)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(y,predicted)\n",
    "ax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\n",
    "ax.set_xlabel('Measured')\n",
    "ax.set_ylabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "In this lab, we learned two widely used linear regression models: Multiple linear regression and Ridge regression. We saw how to apply them to the Boston House Prices dataset. We studied and practiced pyDAAL API for these two algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
